# Template for CommonLLMConnector configuration
# Copy this file, rename it (e.g., to llm_config.yaml), and fill in your credentials.
# Do not commit sensitive information to version control.

logging:
  level: "DEBUG"
  file: "logs/llm.log"

tracking:
  track_tokens: true
  track_cost: true

openai:
  api_key: "your-openai-api-key-here"

ollama:
  api_base: "http://localhost:11434"  # or your custom Ollama endpoint

bedrock:
  aws_credentials:
    access_key_id: "your-aws-access-key-here"
    secret_access_key: "your-aws-secret-access-key-here"
    region: "us-east-1"  # or your preferred AWS region

vllm:
  api_base: "http://your-vllm-endpoint:8000/v1"  # or your custom vLLM endpoint

# Add more providers as needed, following the same structure.
